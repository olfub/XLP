*There are also pdf files in the `figures` folder which are of better quality than the png files shown in the markdown file.*

## Single Feature Attributions

In the image below, feature attributions for both features on the example of the Vertex Distance encoding are shown. These plots have been created using the same experiment as in Figure 3 of the main paper. The LP has two features, one horizontal and one vertical, where the grey lines indicate the constraints for that LP. The line on the top shows the summed up attribution of all features for the Vertex Distance encoding. Each column shows a different attribution method (or configuration). For FP and LIME, $\text{p}$ indicates the maximum possible perturbation in any direction. The (rounded) numbers for the constraints of that LP are shown on the bottom right. The part enclosed in the grey box titled "Feature Attributions" shows attribution for both single features on the Vertex Distance encoding.

Generally, it can be seen how the methods also differ in their single feature attributions and that feature attributions can vary significantly depending on the respective feature.

![Single feature attributions for the Vertex Distance encoding](https://github.com/olfub/XLP/blob/main/figures/single_feature_attributions.png?raw=true)

In the following, we go into detail about the behavior of the attribution methods used in this paper and explain the differences between the attributions for the two features.

### Integrated Gradients
Attribution for IG is obtained by calculating gradients on the path from a baseline to the input vector. As mentioned in the paper, this experiment uses the origin as its baseline: $(0, 0)$. To illustrate this calculation, consider points which only differ from the baseline on the vertical feature (the far left of the shown plots).
Here, the attribution for the horizontal feature remains at $0$, as there is no change of this feature compared to the baseline, so it can not be responsible for any change in the output. Moving up towards the top left vertex however, the vertical feature gets an increasingly larger negative attribution. Since the attributions of IG sum up to the difference in output between baseline and input instance (Completeness), this difference has to be reflected by the feature attributions. In this encoding, the function values encode the distance to the nearest vertex. The encoding at the baseline (origin) has some positive value and the lowest values (the smallest possible distance, i.e. $0$) can be found on any of the vertices. Therefore, the attribution for the vertical feature on the left vertex (where the horizontal feature remains unchanged compared to the baseline) must equal the negative difference between baseline output and $0$, since otherwise the Completeness property would be violated. Another interesting region can be found on the top, roughly in the middle of our plot. Here is the same kind of negative attribution for the vertical feature but also some positive attribution for the horizontal feature. While the increased vertical feature decreases the distance to that vertex, the increased horizontal feature increases the distance which results in positive attribution for that feature. In other words, the horizontal feature being larger than its baseline value ($0$) has a positive (the distance increases) impact on the output. This same reasoning also applies to other regions but there, the attribution can be influenced by other vertices, resulting in different attributions. For example, being on the bottom right indicates a small distance to the nearest vertex there (one of the two vertices on the bottom right), which is why for such instances positive horizontal feature values can have negative attributions.

### Saliency
Since Saliency  uses the local gradient for its attributions, understanding gradients is mostly sufficient for understanding this attribution method. To put it simply, on a specific point, if a feature has a positive impact on the output, then the gradient is positive, and if a feature has a negative impact on the output, then the gradient is negative. This impact and therewith the gradient can also be $0$. Let us consider the Saliency feature attributions in the plots at the start of this file. In accordance with Directedness, if increasing a feature would get the point closer to a vertex (decrease the distance), it gets negative attribution. If increasing a feature would get the point further away from a vertex (increase the distance), it gets positive attribution. Keep in mind that, like with gradients, such statements of "increasing" and "decreasing" should not be interpreted with some specific amount of change but rather infinitesimal changes of the input. The white areas in this figure might benefit from some further explaining. First of all, on and around the vertices, there is (close to) $0$ attribution. The vertices are points where in theory the gradient should be undefined (because of the absolute value function used in the encoding). However, the NN approximates the true underlying function in a continuous way, leading to a continuously changing gradient and an attribution of $0$ on the vertices which rather quickly reaches the ``normal'' gradient when moving away from those vertices in either direction. The white line starting at the bottom (somewhat left) and ending on the top (somewhat right) also has many points with (close to) $0$ attribution because in this region, the gradient switches signs. For the single feature attributions, there are also some white lines moving away from the vertices: to the top/bottom for the horizontal feature and to the left/right for the vertical feature. Those are areas where only that respective feature changes its gradient from one sign to another. For example, if a point is below the nearest vertex, then its vertical feature attribution is negative first and once the point is above the closest vertex, the attribution becomes positive. At some point in between, when the point is on the same height as the vertex, its feature attribution is 0. This is true both if these points are exactly under/above the vertex or slightly on one side. Combining this behavior for points on the left and right of the vertices results in a horizontal white line indicating no attribution next to the vertex. The explanation for the horizontal feature can be done the same way. Such white lines resulting from this type of behavior are not necessarily visible in the plot for the attribution sum, since here the other features can have non-zero attributions. However, the attribution sum can also contain areas with no attribution even though there are single feature attributions because in such situations, these get averaged out to $0$ overall.

### Feature Permutation
The attributions of FP need to be interpreted differently as they are unlike the other attribution methods show in the plots. For FP, the output of the input point is compared to the outputs of neighboring points (perturbations). If, on average, their output is larger, then the attribution of the input instance is negative which describes the behavior seen around the vertices. If the neighboring points are, on average, smaller, then the attribution is positive. For the Vertex Distance encoding, this blue (positive) attribution can be observed on the line which has an equal distance to both nearest vertices, as here perturbing any feature in any direction creates a point closer to a vertex. Note, that FP always considers only one feature changed at a time since perturbations are only created by perturbing a single feature. The single feature attributions also indicate how important the features are compared to each other. For example, on this aforementioned blue line, the attribution is larger for the horizontal feature. This makes sense, as moving to the left or right has a higher impact than moving up or down by the same distance. (Because the line is more vertical than horizontal, changing the horizontal feature can get a point closer to a vertex. In other words, since the vertical distance between the two relevant vertices is smaller than the horizontal distance, changing the horizontal distance here has a higher impact on the output.) The noise visible in many regions is a result of the randomness in the FP perturbations. For example, in many areas the score increases roughly the same in one direction as it decreases in the other direction, therefore the FP attribution should be around $0$. But if due to randomness, perturbations are created more strongly in one direction, the average output is now predominantly influenced by that direction, resulting in an average positive or negative change and an attribution not close to $0$.T his can happen in either direction, which in the plots shown before results in noisy areas with many red and blue dots in an otherwise rather white region.
Interestingly enough, the strength of this noise for both features here indicates in which areas which feature has a higher impact (this even compares to the respective Saliency feature attributions).
The larger the perturbations ($\text{p}$), the broader the area considered for the attributions, resulting in less accurate (local) but increasingly robust attributions that represent more general changes.

### LIME
The similarity between LIME and Saliency is not only present for the attribution sum but also for the single feature attributions. Instead of using the local gradient, LIME creates a small model for the input point based on perturbed instances around it. To summarize the resulting attributions briefly: feature attribution for LIME is positive if larger (smaller) instances have larger (smaller) outputs, negative if larger (smaller) instances have smaller (larger) scores and zero if either their outputs are smaller in one but equally larger in the other direction or if instances around the input have the same output as the input itself. Note, that, as with Saliency, the direction of the change around the input instance matters (Directedness). Also keep in mind that, unlike FP, LIME uses perturbations for multiple features at a time. For larger perturbations, it can be seen that the attribution patterns become more blurred and local details are disappearing. In some situations, this could possibly be an advantage and protect against noise, making the results more robust. There can be some slightly differing attributions of similar points due to the randomness in the perturbations.

## Large-Scale Experiment

### Problem Generation

Using FRaGenLP [1], an algorithm for randomly generating LPs, an LP with 10,000 features and 30 constraints was generated. The LP is generated with instances where all features are between 0 and 100 and the point in the middle (50 for all features) is feasible (i.e. constraints are generated around the point in the middle). Training data for the instances $\mathbf{x}$ were generated in three thirds: a set of evenly distributed feasible instances, a set of evenly distributed infeasible instances, and (because the space of infeasible instances is much more thin in this LP), another third of infeasible instances was generated. For this last third, the difference of an instance to the point we know is feasible (all features are 50) was decreased by 1% until the instance would be feasible. The goal of this data generation procedure is to be able to learn the decision boundary particularly well. However, note that this also results in the generation of many instances which are rather difficult to learn (because they are close to the decision boundary).
2,000,000 instances were used to train the model, namely a NN of 11 layers with 8192 neurons each.
The Feasibility encoding was learned in this experiment. The trained model achieved 81.81% accuracy on the test set which, compared to the 67% baseline of always predicting the infeasible class, has learned a lot but also shows significant errors in the predictions.

### Additional Results
In addition to the results shown in the paper, a set of 10 examples from the evaluation of the large-scale experiment can be found in the images below. Because of the large dimensional of the LP, only selected columns and rows are shown. $\mathbf{A}\mathbf{x}$ was calculated using the full LP, before selecting the depicted rows. The values in $\mathbf{A}\mathbf{x}$ are in the order of $10^4$. The colors in $\mathbf{A}$ serve as an additional indicator of the elements (negative values are dark pink/purple, positive values are green). The model prediction and the true class are written on the left side. All values are rounded.

Feature Permutation and LIME use perturbations of up to 10. For IG and Saliency, the results in large parts fit the patterns also discussed for smaller LPs.
For FP and LIME, the results are not as easily made sense of. Here, the errors still left in the model might play an important role as there are many predictions made for the perturbations which could be false.

![Additional Results for the Large-Scale Experiment 1](https://github.com/olfub/XLP/blob/main/figures/large_lp_appendix_additional_results_1.png?raw=true)

![Additional Results for the Large-Scale Experiment 2](https://github.com/olfub/XLP/blob/main/figures/large_lp_appendix_additional_results_2.png?raw=true)

## Technical Details

All models were build using pytorch and trained on Nvidia DGX-clusters with A-100 40GB GPUs.

## References

[1] Sokolinsky, L.B., & Sokolinskaya, I.M. (2021). Fragenlp: A generator of random linear programming problems for cluster computing systems. International conference on parallel computational technologies (pp. 164–177)